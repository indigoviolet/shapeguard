* ShapeGuard

ShapeGuard is a tool to help with handling shapes in Pytorch, Tensorflow and NumPy.

Forked from https://github.com/Qwlouse/shapeguard.

* Changes

- removed generated lark parser in favor of using the library directly
- removed dotted access to dims from ShapeGuard
- cleaned up Python types
- added a shim system to make it extensible
- added support for Pytorch tensors
- added `sg()` and `Tensor.sg`` patching for Pytorch
- added support for list templates in `sg()`
- added support for assignment in shape templates
- exceptions from sg() display the original line
- Added sg.fork(...) context manager
- Added sg.noop() context manager
- Added sg.install()
- Support floats in templates (to handle `2.0` etc from interpolation)


* TODO

** DONE should dynamic named dimensions be stored?
   if they should, then should there be a syntax for named but
   unstored dimensions (for documentation purposes, to handle
   dimensions that will be different in the future)?
   eg. _num_gt_targets already works!,
** DONE use devtools.debug to produce error message containing the actual tensor name
** DONE with sg_fork(stride=)

   - allow a forked ShapeGuard obj which will create a singleton that
     can be reused later

   - probably need to allow this singleton to update its dims from the
     base singleton (maybe use chainmap?)

   - I think we will want this context manager to activate the forked
     shapeguard for all calls within it

** DONE support floats instead of int (mainly for interpolation after division or multiplication)
** DONE support iterable in sg() "list" mode instead only list
** DONE with sg_noop: context manager

   so that pl.Trainer.tune() can run with different batch sizes etc.

*** TODO support no-op mode for sg()

    +from shapeguard import sg_noop as sg+

    this isn't sufficient because it requires changing imports all over
    the place

** DONE See icecream.install() to add sg to builtins

** Tests
** Display the full function call in the debug frame in sg()

   a multiline call like

   sg(
      foo
   )

   currently only captures the first line


*** idea for this

    use parso? to find the minimal number of lines that parses

** checkin_fork: another idea is if we need a mechanism for dims inferred within a
    fork to propagate up to the base, use uppercase Dims for base,
    and lowercase dims for forked

** another issue: batch size might be smaller on the last batch. so
   we actually need to fork from the base/previous batch, but allow
   B to be changed.


** Add a decorator @sg() that can guard function args
** cache results by tensor id/template?
** [probably not] Allow externally supplied `dim=val` args to `sg()`
   these should be inserted into known_dims before template processing

*** is this better than interpolation of the value?

    - it enters known_dims, which it could if we did Dim={var}
    - we can make sure it's an int (sometimes floats get interpolated)

** DONE +None vs -1 for+ dynamic dimensions

   +convert to common=None,  via shim+

   What are dynamic dimensions anyway?
   https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/

   Don't seem to be relevant to Pytorch, so nothing to do here

* Usage

#+BEGIN_SRC python

## Basic Usage
import tensorflow as tf
from shapeguard import ShapeGuard

sg = ShapeGuard()

img = tf.ones([64, 32, 32, 3])
flat_img = tf.ones([64, 1024])
labels = tf.ones([64])

# check shape consistency
sg.guard(img, "B, H, W, C")
sg.guard(labels, "B, 1")  # raises error because of rank mismatch
sg.guard(flat_img, "B, H*W*C")  # raises error because 1024 != 32*32*3

# guard also returns the tensor, so it can be inlined
mean_img = sg.guard(tf.reduce_mean(img, axis=0), "H, W, C")

# more readable reshapes
flat_img = sg.reshape(img, 'B, H*W*C')

# evaluate templates
assert sg['H, W*C+1'] == [32, 97]

# attribute access to inferred dimensions
assert sg.dims['B'] == 64
#+END_SRC

* Convenient guarding

  #+BEGIN_SRC python

    # Pytorch tensors are monkeypatched with an sg() method for
    # convenience, which calls guard() on a hidden singleton (see below)

    import torch
    import shapeguard               # required for patching

    t = torch.arange(0, 10).unsqueeze(0)
    t.sg('B,W')

    # There is a ShapeGuard singleton that supports the monkeypatched `sg` method
    shapeguard.ShapeGuard.get().dims
    # >>> {'B': 1, 'W': 10}

    # it can be reset
    shapeguard.ShapeGuard.reset()
    shapeguard.ShapeGuard.get().dims
    # >>> {}

  #+END_SRC

  One annoyance with the above is that mypy will complain about
  Tensor not having an `sg` attribute.

  A mypy-safe alternative is to do:

  #+BEGIN_SRC python

    from shapeguard import sg

    sg(t, 'B,W')

  #+END_SRC

  This is ok, except for the annoyance of having to import shapeguard everywhere.

* Shape Template Syntax
  The shape template mini-DSL supports many different ways of specifying shapes:

 - numbers: ~64, 32, 32, 3~
 - named dimensions: ~B, width, height2, channels~
 - assignment to names that can then be used in further guards: ~B, W2=W/2, H, C~
 - wildcards: ~B, *, *, *~
 - ellipsis: ~B, ..., 3~
 - addition, subtraction, multiplication, division: ~B*N, W/2, H*(C+1)~
 - dynamic dimensions: ~?, H, W, C~  (only matches ~[None, H, W, C]~)
 - comment-only dimensions: ~?,_num_targets,W,C~ (~num_targets~ won't be stored for future)
